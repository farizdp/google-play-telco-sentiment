{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Scrape Google Play Reviews\n",
    "\n",
    "Scrape reviews for 3 Indonesian telco apps:\n",
    "- **MyTelkomsel** (`com.telkomsel.telkomselcm`)\n",
    "- **myXL** (`com.apps.MyXL`)\n",
    "- **myIM3** (`com.pure.indosat.care`)\n",
    "\n",
    "**Parameters:** English language, Indonesia country, max 10,000 reviews per app, last 3 months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nfrom google_play_scraper import Sort, reviews\nfrom tqdm import tqdm\nfrom datetime import datetime, timedelta\nimport os\nimport time\nimport re\nimport unicodedata\nfrom langdetect import detect, LangDetectException"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# App configurations\nAPPS = {\n    \"MyTelkomsel\": \"com.telkomsel.telkomselcm\",\n    \"myXL\": \"com.apps.MyXL\",\n    \"myIM3\": \"com.pure.indosat.care\",\n}\n\n# Scraping parameters\nLANG = \"en\"\nCOUNTRY = \"id\"\nMAX_REVIEWS = 10000\nBATCH_SIZE = 200\nDATE_CUTOFF = datetime.now() - timedelta(days=90)  # Last 3 months\n\n# Version/date tag for output files\nRUN_DATE = datetime.now().strftime(\"%Y%m%d\")\n\n# Output directory\nOUTPUT_DIR = \"data/raw\"\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\nprint(f\"Date cutoff: {DATE_CUTOFF.strftime('%Y-%m-%d')}\")\nprint(f\"Max reviews per app: {MAX_REVIEWS}\")\nprint(f\"Output file tag: {RUN_DATE}\")"
  },
  {
   "cell_type": "code",
   "source": "def clean_text(text):\n    \"\"\"Clean review text.\"\"\"\n    if pd.isna(text) or not isinstance(text, str):\n        return \"\"\n    \n    # Normalize unicode characters\n    text = unicodedata.normalize(\"NFKD\", text)\n    \n    # Replace newlines and tabs with space\n    text = re.sub(r\"[\\n\\r\\t]+\", \" \", text)\n    \n    # Remove emojis and special unicode symbols (keep basic punctuation)\n    text = re.sub(\n        r\"[\\U00010000-\\U0010ffff]\",  # Supplementary Unicode planes (emojis etc)\n        \"\",\n        text,\n    )\n    \n    # Remove excessive punctuation repetition (e.g., \"!!!!!!\" -> \"!\")\n    text = re.sub(r\"([!?.])\\1{2,}\", r\"\\1\", text)\n    \n    # Collapse multiple spaces\n    text = re.sub(r\"\\s{2,}\", \" \", text)\n    \n    return text.strip()\n\n\ndef detect_language(text):\n    \"\"\"Detect language of text. Returns language code or 'unknown'.\"\"\"\n    try:\n        return detect(text)\n    except LangDetectException:\n        return \"unknown\"",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_app_reviews(app_name, app_id):\n",
    "    \"\"\"Scrape reviews for a single app with date filtering.\"\"\"\n",
    "    all_reviews = []\n",
    "    token = None\n",
    "    \n",
    "    pbar = tqdm(total=MAX_REVIEWS, desc=f\"Scraping {app_name}\")\n",
    "    \n",
    "    while len(all_reviews) < MAX_REVIEWS:\n",
    "        try:\n",
    "            batch, token = reviews(\n",
    "                app_id,\n",
    "                lang=LANG,\n",
    "                country=COUNTRY,\n",
    "                sort=Sort.NEWEST,\n",
    "                count=BATCH_SIZE,\n",
    "                continuation_token=token,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError fetching {app_name}: {e}\")\n",
    "            break\n",
    "        \n",
    "        if not batch:\n",
    "            print(f\"\\nNo more reviews for {app_name}.\")\n",
    "            break\n",
    "        \n",
    "        # Filter by date\n",
    "        for review in batch:\n",
    "            review_date = review[\"at\"]\n",
    "            if review_date >= DATE_CUTOFF:\n",
    "                all_reviews.append(review)\n",
    "            else:\n",
    "                # Reviews are sorted newest first, so we can stop\n",
    "                pbar.update(pbar.total - pbar.n)  # Fill progress bar\n",
    "                pbar.close()\n",
    "                print(f\"Reached date cutoff for {app_name}. Got {len(all_reviews)} reviews.\")\n",
    "                return all_reviews\n",
    "        \n",
    "        pbar.update(len(batch))\n",
    "        time.sleep(1)  # Rate limiting\n",
    "    \n",
    "    pbar.close()\n",
    "    print(f\"Finished {app_name}. Got {len(all_reviews)} reviews.\")\n",
    "    return all_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape all apps\n",
    "all_data = {}\n",
    "\n",
    "for app_name, app_id in APPS.items():\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Scraping: {app_name} ({app_id})\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    app_reviews = scrape_app_reviews(app_name, app_id)\n",
    "    all_data[app_name] = app_reviews\n",
    "    \n",
    "    print(f\"Total reviews collected: {len(app_reviews)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Convert to DataFrames, clean, and save\nPROCESSED_DIR = \"data/processed\"\nos.makedirs(PROCESSED_DIR, exist_ok=True)\n\ncombined_dfs = []\n\nfor app_name, app_reviews in all_data.items():\n    df = pd.DataFrame(app_reviews)\n    df[\"app_name\"] = app_name\n    \n    # Select useful columns\n    columns = [\n        \"reviewId\", \"userName\", \"content\", \"score\", \"thumbsUpCount\",\n        \"reviewCreatedVersion\", \"at\", \"replyContent\", \"repliedAt\", \"app_name\"\n    ]\n    df = df[[c for c in columns if c in df.columns]]\n    \n    # Preprocessing: drop null/empty content\n    df = df.dropna(subset=[\"content\"])\n    df = df[df[\"content\"].str.strip() != \"\"]\n    \n    # Clean text and detect language\n    df[\"content_clean\"] = df[\"content\"].apply(clean_text)\n    df = df[df[\"content_clean\"].str.len() > 0]\n    \n    print(f\"Detecting language for {app_name}...\")\n    df[\"language\"] = df[\"content_clean\"].apply(detect_language)\n    \n    # Save per-app CSV\n    filename = f\"{OUTPUT_DIR}/{app_name}_reviews_{RUN_DATE}.csv\"\n    df.to_csv(filename, index=False)\n    print(f\"Saved {len(df)} reviews to {filename}\")\n    \n    combined_dfs.append(df)\n\n# Combined DataFrame\ndf_all = pd.concat(combined_dfs, ignore_index=True)\n\n# Remove duplicates by reviewId\nbefore = len(df_all)\ndf_all = df_all.drop_duplicates(subset=\"reviewId\")\nafter = len(df_all)\nif before != after:\n    print(f\"Removed {before - after} duplicate reviews.\")\n\n# Save combined raw (with cleaned columns)\ncombined_file = f\"{OUTPUT_DIR}/all_reviews_{RUN_DATE}.csv\"\ndf_all.to_csv(combined_file, index=False)\nprint(f\"\\nSaved combined {len(df_all)} reviews to {combined_file}\")\n\n# Save processed files (compatible with 03_sentiment_analysis.ipynb)\ncleaned_file = f\"{PROCESSED_DIR}/reviews_cleaned_{RUN_DATE}.csv\"\ndf_all.to_csv(cleaned_file, index=False)\nprint(f\"Saved {len(df_all)} cleaned reviews to {cleaned_file}\")\n\n# Save English-only subset\ndf_en = df_all[df_all[\"language\"] == \"en\"]\nen_file = f\"{PROCESSED_DIR}/reviews_english_{RUN_DATE}.csv\"\ndf_en.to_csv(en_file, index=False)\nprint(f\"Saved {len(df_en)} English reviews to {en_file}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Summary\nprint(\"\\n\" + \"=\"*50)\nprint(\"SCRAPING & PREPROCESSING SUMMARY\")\nprint(\"=\"*50)\nfor app_name in APPS:\n    total = len(df_all[df_all[\"app_name\"] == app_name])\n    en = len(df_en[df_en[\"app_name\"] == app_name])\n    print(f\"{app_name}: {total} total, {en} English\")\n\nprint(f\"\\nTotal: {len(df_all)} reviews\")\nprint(f\"English: {len(df_en)} reviews ({len(df_en)/len(df_all)*100:.1f}%)\")\nprint(f\"Date range: {df_all['at'].min()} to {df_all['at'].max()}\")\nprint(f\"\\nScore distribution:\\n{df_all['score'].value_counts().sort_index()}\")\nprint(f\"\\nLanguage distribution (top 10):\\n{df_all['language'].value_counts().head(10)}\")\nprint(f\"\\nOutput files:\")\nprint(f\"  Raw:     {combined_file}\")\nprint(f\"  Cleaned: {cleaned_file}\")\nprint(f\"  English: {en_file}\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}